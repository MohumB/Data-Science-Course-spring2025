{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JiIkQbhFR7yF"
   },
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "\n",
    "with open(\"/content/David_Copperfield.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    lines = f.readlines()\n",
    "\n",
    "# Remove empty lines and strip\n",
    "lines = [line.strip() for line in lines if line.strip()]\n",
    "\n",
    "# Create a Dataset from a list of dicts\n",
    "dataset = Dataset.from_dict({\"text\": lines})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 319,
     "status": "ok",
     "timestamp": 1749199214367,
     "user": {
      "displayName": "MohammadHossein Barabadi",
      "userId": "08282318677652093529"
     },
     "user_tz": -210
    },
    "id": "Z9O92cTtQqSW",
    "outputId": "cd43def8-f4dc-4a7f-dac9-6cffb9297194"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style> .dataframe th, .dataframe tbody td { text-align: left; padding-right: 30px; } </style> <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>By Charles Dickens.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>This ebook is the product of many hours of hard work by volunteers for Standard Ebooks, and builds on the hard work of other literature lovers made possible by the public domain.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>This particular ebook is based on a transcription from Project Gutenberg and on digital scans from the Internet Archive.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>The source text and artwork in this ebook are believed to be in the United States public domain; that is, they are believed to be free of copyright restrictions in the United States. They may still be copyrighted in other countries, so users located outside of the United States must check their local laws before using this ebook. The creators of, and contributors to, this ebook dedicate their contributions to the worldwide public domain via the terms in the CC0 1.0 Universal Public Domain Dedication. For full license information, see the Uncopyright at the end of this ebook.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Standard Ebooks is a volunteer-driven project that produces ebook editions of public domain literature using modern typography, technology, and editorial standards, and distributes them free of cost. You can download this and other ebooks carefully produced for true book lovers at standardebooks.org.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>I do not find it easy to get sufficiently far away from this book, in the first sensations of having finished it, to refer to it with the composure which this formal heading would seem to require. My interest in it, is so recent and strong; and my mind is so divided between pleasure and regret⁠—pleasure in the achievement of a long design, regret in the separation from many companions⁠—that I am in danger of wearying the reader whom I love, with personal confidences, and private emotions.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Besides which, all that I could say of the story, to any purpose, I have endeavoured to say in it.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>It would concern the reader little, perhaps, to know, how sorrowfully the pen is laid down at the close of a two-years’ imaginative task; or how an author feels as if he were dismissing some portion of himself into the shadowy world, when a crowd of the creatures of his brain are going from him forever. Yet, I have nothing else to tell; unless, indeed, I were to confess (which might be of less moment still) that no one can ever believe this narrative, in the reading, more than I have believed it in the writing.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Instead of looking back, therefore, I will look forward. I cannot close this volume more agreeably to myself, than with a hopeful glance towards the time when I shall again put forth my two green leaves once a month, and with a faithful remembrance of the genial sun and showers that have fallen on these leaves of David Copperfield, and made me happy.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>London, October, 1850.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>I remarked in the original preface to this book, that I did not find it easy to get sufficiently far away from it, in the first sensations of having finished it, to refer to it with the composure which this formal heading would seem to require. My interest in it was so recent and strong, and my mind was so divided between pleasure and regret⁠—pleasure in the achievement of a long design, regret in the separation from many companions⁠—that I was in danger of wearying the reader with personal confidences and private emotions.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Besides which, all that I could have said of the story to any purpose, I had endeavoured to say in it.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>It would concern the reader little, perhaps, to know how sorrowfully the pen is laid down at the close of a two-years’ imaginative task; or how an author feels as if he were dismissing some portion of himself into the shadowy world, when a crowd of the creatures of his brain are going from him forever. Yet, I had nothing else to tell; unless, indeed, I were to confess (which might be of less moment still), that no one can ever believe this narrative, in the reading, more than I believed it in the writing.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>So true are these avowals at the present day, that I can now only take the reader into one confidence more. Of all my books, I like this the best. It will be easily believed that I am a fond parent to every child of my fancy, and that no one can ever love that family as dearly as I love them. But, like many fond parents, I have in my heart of hearts a favourite child. And his name is</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>David Copperfield</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>1869</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Affectionately inscribed to</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>the Hon. Mr. and Mrs. Richard Watson,</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>of Rockinghan, Northamptonshire.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>I Am Born</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>Whether I shall turn out to be the hero of my own life, or whether that station will be held by anybody else, these pages must show. To begin my life with the beginning of my life, I record that I was born (as I have been informed and believe) on a Friday, at twelve o’clock at night. It was remarked that the clock began to strike, and I began to cry, simultaneously.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>In consideration of the day and hour of my birth, it was declared by the nurse, and by some sage women in the neighbourhood who had taken a lively interest in me several months before there was any possibility of our becoming personally acquainted, first, that I was destined to be unlucky in life; and secondly, that I was privileged to see ghosts and spirits; both these gifts inevitably attaching, as they believed, to all unlucky infants of either gender, born towards the small hours on a Friday night.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>I need say nothing here, on the first head, because nothing can show better than my history whether that prediction was verified or falsified by the result. On the second branch of the question, I will only remark, that unless I ran through that part of my inheritance while I was still a baby, I have not come into it yet. But I do not at all complain of having been kept out of this property; and if anybody else should be in the present enjoyment of it, he is heartily welcome to keep it.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>I was born with a caul, which was advertised for sale, in the newspapers, at the low price of fifteen guineas. Whether seagoing people were short of money about that time, or were short of faith and preferred cork jackets, I don’t know; all I know is, that there was but one solitary bidding, and that was from an attorney connected with the bill-broking business, who offered two pounds in cash, and the balance in sherry, but declined to be guaranteed from drowning on any higher bargain. Consequently the advertisement was withdrawn at a dead loss⁠—for as to sherry, my poor dear mother’s own sherry was in the market then⁠—and ten years afterwards, the caul was put up in a raffle down in our part of the country, to fifty members at half-a-crown a head, the winner to spend five shillings. I was present myself, and I remember to have felt quite uncomfortable and confused, at a part of myself being disposed of in that way. The caul was won, I recollect, by an old lady with a hand-basket, who, very reluctantly, produced from it the stipulated five shillings, all in halfpence, and twopence halfpenny short⁠—as it took an immense time and a great waste of arithmetic, to endeavour without any effect to prove to her. It is a fact which will be long remembered as remarkable down there, that she was never drowned, but died triumphantly in bed, at ninety-two. I have understood that it was, to the last, her proudest boast, that she never had been on the water in her life, except upon a bridge; and that over her tea (to which she was extremely partial) she, to the last, expressed her indignation at the impiety of mariners and others, who had the presumption to go “meandering” about the world. It was in vain to represent to her that some conveniences, tea perhaps included, resulted from this objectionable practice. She always returned, with greater emphasis and with an instinctive knowledge of the strength of her objection, “Let us have no meandering.”</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>Not to meander myself, at present, I will go back to my birth.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>I was born at Blunderstone, in Suffolk, or “there by,” as they say in Scotland. I was a posthumous child. My father’s eyes had closed upon the light of this world six months, when mine opened on it. There is something strange to me, even now, in the reflection that he never saw me; and something stranger yet in the shadowy remembrance that I have of my first childish associations with his white gravestone in the churchyard, and of the indefinable compassion I used to feel for it lying out alone there in the dark night, when our little parlour was warm and bright with fire and candle, and the doors of our house were⁠—almost cruelly, it seemed to me sometimes⁠—bolted and locked against it.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>An aunt of my father’s, and consequently a great-aunt of mine, of whom I shall have more to relate by and by, was the principal magnate of our family. Miss Trotwood, or Miss Betsey, as my poor mother always called her, when she sufficiently overcame her dread of this formidable personage to mention her at all (which was seldom), had been married to a husband younger than herself, who was very handsome, except in the sense of the homely adage, “handsome is, that handsome does”⁠—for he was strongly suspected of having beaten Miss Betsey, and even of having once, on a disputed question of supplies, made some hasty but determined arrangements to throw her out of a two pair of stairs’ window. These evidences of an incompatibility of temper induced Miss Betsey to pay him off, and effect a separation by mutual consent. He went to India with his capital, and there, according to a wild legend in our family, he was once seen riding on an elephant, in company with a Baboon; but I think it must have been a Baboo⁠—or a Begum. Anyhow, from India tidings of his death reached home, within ten years. How they affected my aunt, nobody knew; for immediately upon the separation, she took her maiden name again, bought a cottage in a hamlet on the seacoast a long way off, established herself there as a single woman with one servant, and was understood to live secluded, ever afterwards, in an inflexible retirement.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>My father had once been a favourite of hers, I believe; but she was mortally affronted by his marriage, on the ground that my mother was “a wax doll.” She had never seen my mother, but she knew her to be not yet twenty. My father and Miss Betsey never met again. He was double my mother’s age when he married, and of but a delicate constitution. He died a year afterwards, and, as I have said, six months before I came into the world.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>This was the state of matters, on the afternoon of, what I may be excused for calling, that eventful and important Friday. I can make no claim therefore to have known, at that time, how matters stood; or to have any remembrance, founded on the evidence of my own senses, of what follows.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>My mother was sitting by the fire, but poorly in health, and very low in spirits, looking at it through her tears, and desponding heavily about herself and the fatherless little stranger, who was already welcomed by some grosses of prophetic pins, in a drawer upstairs, to a world not at all excited on the subject of his arrival; my mother, I say, was sitting by the fire, that bright, windy March afternoon, very timid and sad, and very doubtful of ever coming alive out of the trial that was before her, when, lifting her eyes as she dried them, to the window opposite, she saw a strange lady coming up the garden.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>My mother had a sure foreboding at the second glance, that it was Miss Betsey. The setting sun was glowing on the strange lady, over the garden-fence, and she came walking up to the door with a fell rigidity of figure and composure of countenance that could have belonged to nobody else.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>When she reached the house, she gave another proof of her identity. My father had often hinted that she seldom conducted herself like any ordinary Christian; and now, instead of ringing the bell, she came and looked in at that identical window, pressing the end of her nose against the glass to that extent, that my poor dear mother used to say it became perfectly flat and white in a moment.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>She gave my mother such a turn, that I have always been convinced I am indebted to Miss Betsey for having been born on a Friday.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>My mother had left her chair in her agitation, and gone behind it in the corner. Miss Betsey, looking round the room, slowly and inquiringly, began on the other side, and carried her eyes on, like a Saracen’s Head in a Dutch clock, until they reached my mother. Then she made a frown and a gesture to my mother, like one who was accustomed to be obeyed, to come and open the door. My mother went.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>“Mrs. David Copperfield, I think,” said Miss Betsey; the emphasis referring, perhaps, to my mother’s mourning weeds, and her condition.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>“Yes,” said my mother, faintly.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>“Miss Trotwood,” said the visitor. “You have heard of her, I dare say?”</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>My mother answered she had had that pleasure. And she had a disagreeable consciousness of not appearing to imply that it had been an overpowering pleasure.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>“Now you see her,” said Miss Betsey. My mother bent her head, and begged her to walk in.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>They went into the parlour my mother had come from, the fire in the best room on the other side of the passage not being lighted⁠—not having been lighted, indeed, since my father’s funeral; and when they were both seated, and Miss Betsey said nothing, my mother, after vainly trying to restrain herself, began to cry. “Oh tut, tut, tut!” said Miss Betsey, in a hurry. “Don’t do that! Come, come!”</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from datasets import load_dataset\n",
    "from IPython.display import HTML, display\n",
    "\n",
    "def display_table(dataset_or_sample):\n",
    "  # A helper fuction to display a Transformer dataset or single sample contains multi-line string nicely\n",
    "  pd.set_option(\"display.max_colwidth\", None)\n",
    "  pd.set_option(\"display.width\", None)\n",
    "  pd.set_option(\"display.max_rows\", None)\n",
    "\n",
    "  if isinstance(dataset_or_sample, dict):\n",
    "      df = pd.DataFrame(dataset_or_sample, index=[0])\n",
    "  else:\n",
    "      df = pd.DataFrame(dataset_or_sample)\n",
    "\n",
    "  html = df.to_html()\n",
    "  styled_html = f\"\"\"<style> .dataframe th, .dataframe tbody td {{ text-align: left; padding-right: 30px; }} </style> {html}\"\"\"\n",
    "  display(HTML(styled_html))\n",
    "\n",
    "\n",
    "display_table(dataset.select(range(40)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 2544111,
     "status": "ok",
     "timestamp": 1749203497159,
     "user": {
      "displayName": "MohammadHossein Barabadi",
      "userId": "08282318677652093529"
     },
     "user_tz": -210
    },
    "id": "WidO9DvYbhfi",
    "outputId": "ee848ac6-9d70-4d8e-95ef-2b72f6ab12d9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== LSTM Next-Word Prediction for David Copperfield ===\n",
      "\n",
      "Starting training process...\n",
      "Reading text from David_Copperfield.txt...\n",
      "Loaded text with 1933907 characters\n",
      "Preprocessing text...\n",
      "Total words after preprocessing: 300034\n",
      "Creating sequences...\n",
      "Created 300024 training sequences\n",
      "Building vocabulary...\n",
      "Input shape: (300024, 10)\n",
      "Output shape: (300024,)\n",
      "Actual vocabulary size: 5000\n",
      "Building model...\n",
      "Model architecture:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_1\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential_1\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ embedding_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)         │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ lstm_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                   │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ ?                      │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ embedding_1 (\u001b[38;5;33mEmbedding\u001b[0m)         │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ lstm_1 (\u001b[38;5;33mLSTM\u001b[0m)                   │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_1 (\u001b[38;5;33mDropout\u001b[0m)             │ ?                      │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                 │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training...\n",
      "Epoch 1/10\n",
      "\u001b[1m4220/4220\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m246s\u001b[0m 57ms/step - accuracy: 0.1035 - loss: 5.9412 - val_accuracy: 0.1675 - val_loss: 5.2451\n",
      "Epoch 2/10\n",
      "\u001b[1m4220/4220\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m264s\u001b[0m 58ms/step - accuracy: 0.1619 - loss: 5.1479 - val_accuracy: 0.1828 - val_loss: 5.0348\n",
      "Epoch 3/10\n",
      "\u001b[1m4220/4220\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m241s\u001b[0m 57ms/step - accuracy: 0.1755 - loss: 4.9123 - val_accuracy: 0.1909 - val_loss: 4.9456\n",
      "Epoch 4/10\n",
      "\u001b[1m4220/4220\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m259s\u001b[0m 56ms/step - accuracy: 0.1824 - loss: 4.7737 - val_accuracy: 0.1928 - val_loss: 4.8951\n",
      "Epoch 5/10\n",
      "\u001b[1m4220/4220\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m268s\u001b[0m 58ms/step - accuracy: 0.1856 - loss: 4.6613 - val_accuracy: 0.1964 - val_loss: 4.8692\n",
      "Epoch 6/10\n",
      "\u001b[1m4220/4220\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m261s\u001b[0m 58ms/step - accuracy: 0.1912 - loss: 4.5826 - val_accuracy: 0.1957 - val_loss: 4.8633\n",
      "Epoch 7/10\n",
      "\u001b[1m4220/4220\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m258s\u001b[0m 57ms/step - accuracy: 0.1918 - loss: 4.5303 - val_accuracy: 0.1980 - val_loss: 4.8537\n",
      "Epoch 8/10\n",
      "\u001b[1m4220/4220\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m261s\u001b[0m 56ms/step - accuracy: 0.1958 - loss: 4.4619 - val_accuracy: 0.2002 - val_loss: 4.8556\n",
      "Epoch 9/10\n",
      "\u001b[1m4220/4220\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m239s\u001b[0m 57ms/step - accuracy: 0.1989 - loss: 4.4071 - val_accuracy: 0.2007 - val_loss: 4.8516\n",
      "Epoch 10/10\n",
      "\u001b[1m4220/4220\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m239s\u001b[0m 57ms/step - accuracy: 0.1992 - loss: 4.3745 - val_accuracy: 0.2005 - val_loss: 4.8610\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training completed!\n",
      "\n",
      "=== Training completed! ===\n",
      "Model saved to lstm_word_predictor.h5\n",
      "Tokenizer saved to tokenizer.pkl\n",
      "\n",
      "=== Testing the model ===\n",
      "\n",
      "Seed: 'I was born on a'\n",
      "Top predictions:\n",
      "  <OOV>: 0.336\n",
      "  little: 0.025\n",
      "  few: 0.021\n",
      "\n",
      "Seed: 'The old man walked'\n",
      "Top predictions:\n",
      "  out: 0.134\n",
      "  up: 0.121\n",
      "  away: 0.092\n",
      "\n",
      "Seed: 'She looked at me with'\n",
      "Top predictions:\n",
      "  a: 0.300\n",
      "  the: 0.121\n",
      "  his: 0.105\n",
      "\n",
      "Seed: 'In the morning I would'\n",
      "Top predictions:\n",
      "  have: 0.329\n",
      "  be: 0.075\n",
      "  not: 0.074\n",
      "\n",
      "=== Text Generation Examples ===\n",
      "\n",
      "Generated from 'I was born on a':\n",
      "'i was born on a good deal of <OOV> but i feel now as i could even have had had'\n",
      "\n",
      "Generated from 'The old man walked':\n",
      "'the old man walked back to the <OOV> without telling me when i lived at all times in the'\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Embedding, Dropout\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "import pickle\n",
    "import re\n",
    "from collections import Counter\n",
    "\n",
    "class LSTMWordPredictor:\n",
    "    def __init__(self, sequence_length=10, vocab_size=10000, embedding_dim=100, lstm_units=128):\n",
    "        \"\"\"\n",
    "        Initialize the LSTM word predictor with configurable parameters.\n",
    "\n",
    "        sequence_length: How many previous words to use for prediction\n",
    "        vocab_size: Maximum number of unique words to keep in vocabulary\n",
    "        embedding_dim: Dimension of word embeddings\n",
    "        lstm_units: Number of LSTM units in the hidden layer\n",
    "        \"\"\"\n",
    "        self.sequence_length = sequence_length\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.lstm_units = lstm_units\n",
    "        self.model = None\n",
    "        self.tokenizer = None\n",
    "\n",
    "    def preprocess_text(self, text):\n",
    "        \"\"\"\n",
    "        Clean and preprocess the input text for training.\n",
    "        This step is crucial for good model performance.\n",
    "        \"\"\"\n",
    "        # Convert to lowercase for consistency\n",
    "        text = text.lower()\n",
    "\n",
    "        # Replace multiple spaces with single space\n",
    "        text = re.sub(r'\\s+', ' ', text)\n",
    "\n",
    "        # Keep only letters, spaces, and basic punctuation\n",
    "        text = re.sub(r'[^a-zA-Z\\s\\.\\,\\!\\?\\;\\:]', '', text)\n",
    "\n",
    "        # Split into sentences and then words\n",
    "        sentences = re.split(r'[.!?]+', text)\n",
    "        words = []\n",
    "\n",
    "        for sentence in sentences:\n",
    "            sentence_words = sentence.strip().split()\n",
    "            if len(sentence_words) > self.sequence_length:  # Only keep longer sentences\n",
    "                words.extend(sentence_words)\n",
    "\n",
    "        return words\n",
    "\n",
    "    def create_sequences(self, words):\n",
    "        \"\"\"\n",
    "        Create input-output pairs for training the LSTM.\n",
    "        Each sequence of 'sequence_length' words predicts the next word.\n",
    "        \"\"\"\n",
    "        sequences = []\n",
    "        next_words = []\n",
    "\n",
    "        # Create overlapping sequences\n",
    "        for i in range(len(words) - self.sequence_length):\n",
    "            # Input: sequence of words\n",
    "            seq = words[i:i + self.sequence_length]\n",
    "            # Output: the next word\n",
    "            next_word = words[i + self.sequence_length]\n",
    "\n",
    "            sequences.append(seq)\n",
    "            next_words.append(next_word)\n",
    "\n",
    "        return sequences, next_words\n",
    "\n",
    "    def prepare_data(self, text):\n",
    "        \"\"\"\n",
    "        Complete data preparation pipeline from raw text to model-ready arrays.\n",
    "        \"\"\"\n",
    "        print(\"Preprocessing text...\")\n",
    "        words = self.preprocess_text(text)\n",
    "        print(f\"Total words after preprocessing: {len(words)}\")\n",
    "\n",
    "        print(\"Creating sequences...\")\n",
    "        sequences, next_words = self.create_sequences(words)\n",
    "        print(f\"Created {len(sequences)} training sequences\")\n",
    "\n",
    "        # Initialize and fit tokenizer on all words\n",
    "        print(\"Building vocabulary...\")\n",
    "        all_words = words  # Use all words for vocabulary\n",
    "        self.tokenizer = Tokenizer(num_words=self.vocab_size, oov_token=\"<OOV>\")\n",
    "        self.tokenizer.fit_on_texts([all_words])\n",
    "\n",
    "        # Convert sequences to numbers\n",
    "        sequences_encoded = self.tokenizer.texts_to_sequences(sequences)\n",
    "        next_words_encoded = self.tokenizer.texts_to_sequences([[word] for word in next_words])\n",
    "        next_words_encoded = [seq[0] if seq else 0 for seq in next_words_encoded]\n",
    "\n",
    "        # Convert to numpy arrays\n",
    "        X = np.array(sequences_encoded)\n",
    "        y = np.array(next_words_encoded)\n",
    "\n",
    "        # Convert target to categorical (one-hot encoding)\n",
    "        actual_vocab_size = min(self.vocab_size, len(self.tokenizer.word_index) + 1)\n",
    "        #y = to_categorical(y, num_classes=actual_vocab_size)\n",
    "\n",
    "        print(f\"Input shape: {X.shape}\")\n",
    "        print(f\"Output shape: {y.shape}\")\n",
    "        print(f\"Actual vocabulary size: {actual_vocab_size}\")\n",
    "\n",
    "        return X, y, actual_vocab_size\n",
    "\n",
    "    def build_model(self, actual_vocab_size):\n",
    "        \"\"\"\n",
    "        Build the LSTM neural network architecture.\n",
    "        This is where the magic happens - the model learns patterns in word sequences.\n",
    "        \"\"\"\n",
    "        self.model = Sequential([\n",
    "            # Embedding layer: converts word indices to dense vectors\n",
    "            # This learns meaningful representations for each word\n",
    "            Embedding(actual_vocab_size, self.embedding_dim,\n",
    "                     input_length=self.sequence_length),\n",
    "\n",
    "            # LSTM layer: the core of our model\n",
    "            # It learns to remember relevant information from the sequence\n",
    "            LSTM(self.lstm_units, dropout=0.2, recurrent_dropout=0.2),\n",
    "\n",
    "            # Dropout for regularization to prevent overfitting\n",
    "            Dropout(0.3),\n",
    "\n",
    "            # Dense output layer: predicts probability for each word in vocabulary\n",
    "            Dense(actual_vocab_size, activation='softmax')\n",
    "        ])\n",
    "\n",
    "        loss = tf.keras.losses.SparseCategoricalCrossentropy()\n",
    "\n",
    "        # Compile with appropriate loss function for multi-class classification\n",
    "        self.model.compile(\n",
    "            loss=loss,\n",
    "            optimizer='adam',\n",
    "            metrics=['accuracy']\n",
    "        )\n",
    "\n",
    "        print(\"Model architecture:\")\n",
    "        self.model.summary()\n",
    "\n",
    "        return self.model\n",
    "\n",
    "    def train(self, text_file_path, epochs=50, batch_size=128, validation_split=0.1):\n",
    "        \"\"\"\n",
    "        Complete training pipeline from text file to trained model.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Read the text file\n",
    "            print(f\"Reading text from {text_file_path}...\")\n",
    "            with open(text_file_path, 'r', encoding='utf-8') as file:\n",
    "                text = file.read()\n",
    "\n",
    "            print(f\"Loaded text with {len(text)} characters\")\n",
    "\n",
    "            # Prepare training data\n",
    "            X, y, actual_vocab_size = self.prepare_data(text)\n",
    "\n",
    "            # Build model\n",
    "            print(\"Building model...\")\n",
    "            self.build_model(actual_vocab_size)\n",
    "\n",
    "            # Train the model\n",
    "            print(\"Starting training...\")\n",
    "            history = self.model.fit(\n",
    "                X, y,\n",
    "                batch_size=batch_size,\n",
    "                epochs=epochs,\n",
    "                validation_split=validation_split,\n",
    "                verbose=1\n",
    "            )\n",
    "\n",
    "            print(\"Training completed!\")\n",
    "            return history\n",
    "\n",
    "        except FileNotFoundError:\n",
    "            print(f\"Error: Could not find file '{text_file_path}'\")\n",
    "            print(\"Please make sure the David_Copperfield.txt file is in the same directory.\")\n",
    "            return None\n",
    "        except Exception as e:\n",
    "            print(f\"An error occurred during training: {e}\")\n",
    "            return None\n",
    "\n",
    "    def predict_next_word(self, seed_text, num_predictions=5):\n",
    "        \"\"\"\n",
    "        Predict the next word given a seed text.\n",
    "        Returns the top predictions with their probabilities.\n",
    "        \"\"\"\n",
    "        if not self.model or not self.tokenizer:\n",
    "            print(\"Model not trained yet. Please train the model first.\")\n",
    "            return []\n",
    "\n",
    "        # Preprocess the seed text the same way as training data\n",
    "        words = seed_text.lower().split()\n",
    "\n",
    "        # Take the last 'sequence_length' words\n",
    "        if len(words) >= self.sequence_length:\n",
    "            sequence = words[-self.sequence_length:]\n",
    "        else:\n",
    "            # Pad with zeros if not enough words\n",
    "            sequence = [''] * (self.sequence_length - len(words)) + words\n",
    "\n",
    "        # Convert to numbers\n",
    "        sequence_encoded = self.tokenizer.texts_to_sequences([sequence])[0]\n",
    "\n",
    "        # Pad sequence to required length\n",
    "        sequence_padded = pad_sequences([sequence_encoded],\n",
    "                                      maxlen=self.sequence_length,\n",
    "                                      padding='pre')\n",
    "\n",
    "        # Get predictions\n",
    "        predictions = self.model.predict(sequence_padded, verbose=0)[0]\n",
    "\n",
    "        # Get top predictions\n",
    "        top_indices = np.argsort(predictions)[-num_predictions:][::-1]\n",
    "\n",
    "        # Convert back to words\n",
    "        results = []\n",
    "        reverse_word_map = {v: k for k, v in self.tokenizer.word_index.items()}\n",
    "\n",
    "        for idx in top_indices:\n",
    "            if idx in reverse_word_map:\n",
    "                word = reverse_word_map[idx]\n",
    "                probability = predictions[idx]\n",
    "                results.append((word, probability))\n",
    "\n",
    "        return results\n",
    "\n",
    "    def generate_text(self, seed_text, num_words=20, temperature=1.0):\n",
    "        \"\"\"\n",
    "        Generate a sequence of text by repeatedly predicting next words.\n",
    "        Temperature controls randomness: lower = more predictable, higher = more creative.\n",
    "        \"\"\"\n",
    "        if not self.model or not self.tokenizer:\n",
    "            print(\"Model not trained yet. Please train the model first.\")\n",
    "            return seed_text\n",
    "\n",
    "        generated = seed_text.lower().split()\n",
    "\n",
    "        for _ in range(num_words):\n",
    "            # Get the sequence for prediction\n",
    "            if len(generated) >= self.sequence_length:\n",
    "                sequence = generated[-self.sequence_length:]\n",
    "            else:\n",
    "                sequence = [''] * (self.sequence_length - len(generated)) + generated\n",
    "\n",
    "            # Encode and predict\n",
    "            sequence_encoded = self.tokenizer.texts_to_sequences([sequence])[0]\n",
    "            sequence_padded = pad_sequences([sequence_encoded],\n",
    "                                          maxlen=self.sequence_length,\n",
    "                                          padding='pre')\n",
    "\n",
    "            predictions = self.model.predict(sequence_padded, verbose=0)[0]\n",
    "\n",
    "            # Apply temperature scaling for creativity control\n",
    "            predictions = np.log(predictions + 1e-8) / temperature\n",
    "            predictions = np.exp(predictions)\n",
    "            predictions = predictions / np.sum(predictions)\n",
    "\n",
    "            # Sample from the probability distribution\n",
    "            next_index = np.random.choice(len(predictions), p=predictions)\n",
    "\n",
    "            # Convert back to word\n",
    "            reverse_word_map = {v: k for k, v in self.tokenizer.word_index.items()}\n",
    "            if next_index in reverse_word_map:\n",
    "                next_word = reverse_word_map[next_index]\n",
    "                generated.append(next_word)\n",
    "            else:\n",
    "                break  # Stop if we can't find the word\n",
    "\n",
    "        return ' '.join(generated)\n",
    "\n",
    "    def save_model(self, model_path='lstm_word_predictor.h5', tokenizer_path='tokenizer.pkl'):\n",
    "        \"\"\"Save the trained model and tokenizer for later use.\"\"\"\n",
    "        if self.model:\n",
    "            self.model.save(model_path)\n",
    "            print(f\"Model saved to {model_path}\")\n",
    "\n",
    "        if self.tokenizer:\n",
    "            with open(tokenizer_path, 'wb') as f:\n",
    "                pickle.dump(self.tokenizer, f)\n",
    "            print(f\"Tokenizer saved to {tokenizer_path}\")\n",
    "\n",
    "    def load_model(self, model_path='lstm_word_predictor.h5', tokenizer_path='tokenizer.pkl'):\n",
    "        \"\"\"Load a previously trained model and tokenizer.\"\"\"\n",
    "        try:\n",
    "            self.model = tf.keras.models.load_model(model_path)\n",
    "            print(f\"Model loaded from {model_path}\")\n",
    "\n",
    "            with open(tokenizer_path, 'rb') as f:\n",
    "                self.tokenizer = pickle.load(f)\n",
    "            print(f\"Tokenizer loaded from {tokenizer_path}\")\n",
    "\n",
    "            return True\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading model: {e}\")\n",
    "            return False\n",
    "\n",
    "\n",
    "# Example usage and demonstration\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"=== LSTM Next-Word Prediction for David Copperfield ===\\n\")\n",
    "\n",
    "    # Initialize the predictor\n",
    "    predictor = LSTMWordPredictor(\n",
    "        sequence_length=10,  # Use 10 words to predict the next one\n",
    "        vocab_size=5000,     # Keep top 5000 most common words\n",
    "        embedding_dim=100,   # 100-dimensional word embeddings\n",
    "        lstm_units=128       # 128 LSTM units\n",
    "    )\n",
    "\n",
    "    # Train the model\n",
    "    print(\"Starting training process...\")\n",
    "    history = predictor.train('David_Copperfield.txt', epochs=10, batch_size=64)\n",
    "\n",
    "    if history:\n",
    "        print(\"\\n=== Training completed! ===\")\n",
    "\n",
    "        # Save the model\n",
    "        predictor.save_model()\n",
    "\n",
    "        # Test predictions\n",
    "        print(\"\\n=== Testing the model ===\")\n",
    "\n",
    "        test_phrases = [\n",
    "            \"I was born on a\",\n",
    "            \"The old man walked\",\n",
    "            \"She looked at me with\",\n",
    "            \"In the morning I would\"\n",
    "        ]\n",
    "\n",
    "        for phrase in test_phrases:\n",
    "            print(f\"\\nSeed: '{phrase}'\")\n",
    "            predictions = predictor.predict_next_word(phrase, num_predictions=3)\n",
    "            print(\"Top predictions:\")\n",
    "            for word, prob in predictions:\n",
    "                print(f\"  {word}: {prob:.3f}\")\n",
    "\n",
    "        # Generate some text\n",
    "        print(\"\\n=== Text Generation Examples ===\")\n",
    "        for phrase in test_phrases[:2]:  # Just first two for brevity\n",
    "            generated = predictor.generate_text(phrase, num_words=15, temperature=0.8)\n",
    "            print(f\"\\nGenerated from '{phrase}':\")\n",
    "            print(f\"'{generated}'\")\n",
    "\n",
    "    else:\n",
    "        print(\"Training failed. Please check that David_Copperfield.txt is available.\")\n",
    "        print(\"\\nTo use this model:\")\n",
    "        print(\"1. Download David Copperfield text file\")\n",
    "        print(\"2. Place it in the same directory as this script\")\n",
    "        print(\"3. Run the script again\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 14042,
     "status": "ok",
     "timestamp": 1749204378104,
     "user": {
      "displayName": "MohammadHossein Barabadi",
      "userId": "08282318677652093529"
     },
     "user_tz": -210
    },
    "id": "Dg9SaMpxrz9-",
    "outputId": "09c07031-9372-4da8-e3fc-326693ed3677"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Generated from 'My deepest sorrows are':\n",
      "'my deepest sorrows are not very much attached it to see the little <OOV> of my <OOV> take possession of the <OOV> of my <OOV> that <OOV> like, for another one of her lips, and she was a small outside table i am sure, but to <OOV> the subject of her <OOV> and to i told you, i should wish to feel it made the money into the <OOV> of my aunts having been sometimes to <OOV> he made a <OOV> of the fire of a <OOV> and the <OOV> <OOV> <OOV> the <OOV> <OOV> of mr jack <OOV> which was a too, and he had gone to mr peggotty, and mr i told him well, and i could <OOV> my throat, to be got i the time when i first came'\n"
     ]
    }
   ],
   "source": [
    "phrase= \"My deepest sorrows are\"\n",
    "generated = predictor.generate_text(phrase, num_words=128, temperature=0.8)\n",
    "print(f\"\\nGenerated from '{phrase}':\")\n",
    "print(f\"'{generated}'\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyMy8dyQ3KGy6Z9LCHDojc4d",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
