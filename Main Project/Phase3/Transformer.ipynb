{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 394303,
     "status": "ok",
     "timestamp": 1749203619457,
     "user": {
      "displayName": "MohammadHossein Barabadi",
      "userId": "08282318677652093529"
     },
     "user_tz": -210
    },
    "id": "CHYeOuFijkgZ",
    "outputId": "5102f41f-430a-4936-ca6b-d00a04886dbd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Main function running, will use device: cuda\n",
      "Vocabulary size: 8790\n",
      "Training samples: 454098\n",
      "Model parameters: 7,668,822\n",
      "Model is on device: cuda:0\n",
      "\n",
      "Starting training...\n",
      "Epoch 1/1, Average Loss: 3.0255\n",
      "\n",
      "Generating text...\n",
      "Generated text: david , ” said mr . spenlow . “ what can you believe , ” said i , “ what shall i do without you ? ” replied mr . spenlow . i looked aside , as he looked alternately on us downstairs . “ we shall see the <unk> of\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import re\n",
    "from collections import Counter\n",
    "import math\n",
    "import random\n",
    "\n",
    "# Determine the device and print it\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_model, n_heads):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.n_heads = n_heads\n",
    "        self.d_k = d_model // n_heads\n",
    "\n",
    "        self.w_q = nn.Linear(d_model, d_model)\n",
    "        self.w_k = nn.Linear(d_model, d_model)\n",
    "        self.w_v = nn.Linear(d_model, d_model)\n",
    "        self.w_o = nn.Linear(d_model, d_model)\n",
    "\n",
    "    def forward(self, x, mask=None):\n",
    "        batch_size, seq_len, d_model = x.size()\n",
    "\n",
    "        # Linear transformations and split into heads\n",
    "        # Q, K, V: (batch_size, n_heads, seq_len, d_k)\n",
    "        Q = self.w_q(x).view(batch_size, seq_len, self.n_heads, self.d_k).transpose(1, 2)\n",
    "        K = self.w_k(x).view(batch_size, seq_len, self.n_heads, self.d_k).transpose(1, 2)\n",
    "        V = self.w_v(x).view(batch_size, seq_len, self.n_heads, self.d_k).transpose(1, 2)\n",
    "\n",
    "        # Scaled dot-product attention\n",
    "        # scores: (batch_size, n_heads, seq_len, seq_len)\n",
    "        scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(self.d_k)\n",
    "\n",
    "        if mask is not None:\n",
    "            # mask is typically (1, 1, seq_len, seq_len) or (batch_size, 1, seq_len, seq_len)\n",
    "            # It must be on the same device as scores.\n",
    "            # masked_fill fills elements where mask == 0 is True.\n",
    "            scores = scores.masked_fill(mask == 0, -1e9)\n",
    "\n",
    "        attention_weights = F.softmax(scores, dim=-1)\n",
    "        attention_output = torch.matmul(attention_weights, V)\n",
    "\n",
    "        # Concatenate heads and put through final linear layer\n",
    "        # attention_output: (batch_size, seq_len, d_model)\n",
    "        attention_output = attention_output.transpose(1, 2).contiguous().view(\n",
    "            batch_size, seq_len, d_model)\n",
    "\n",
    "        return self.w_o(attention_output)\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, d_model, d_ff):\n",
    "        super().__init__()\n",
    "        self.linear1 = nn.Linear(d_model, d_ff)\n",
    "        self.linear2 = nn.Linear(d_ff, d_model)\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.linear2(self.dropout(F.relu(self.linear1(x))))\n",
    "\n",
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, d_model, n_heads, d_ff):\n",
    "        super().__init__()\n",
    "        self.attention = MultiHeadAttention(d_model, n_heads)\n",
    "        self.feed_forward = FeedForward(d_model, d_ff)\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "\n",
    "    def forward(self, x, mask=None):\n",
    "        # Self-attention with residual connection and layer norm\n",
    "        attn_output = self.attention(x, mask)\n",
    "        x = self.norm1(x + self.dropout(attn_output))\n",
    "\n",
    "        # Feed forward with residual connection and layer norm\n",
    "        ff_output = self.feed_forward(x)\n",
    "        x = self.norm2(x + self.dropout(ff_output))\n",
    "\n",
    "        return x\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, max_seq_length=5000):\n",
    "        super().__init__()\n",
    "\n",
    "        pe = torch.zeros(max_seq_length, d_model)\n",
    "        position = torch.arange(0, max_seq_length, dtype=torch.float).unsqueeze(1)\n",
    "\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() *\n",
    "                           (-math.log(10000.0) / d_model))\n",
    "\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "\n",
    "        # pe is (max_seq_length, d_model)\n",
    "        # self.register_buffer makes 'pe' part of model's state_dict\n",
    "        # and moves it to GPU if model.to(device) is called.\n",
    "        self.register_buffer('pe', pe.unsqueeze(0)) # (1, max_seq_length, d_model)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x is (batch_size, seq_len, d_model)\n",
    "        # self.pe is (1, max_seq_length, d_model)\n",
    "        # self.pe[:, :x.size(1)] is (1, seq_len, d_model), will broadcast with x\n",
    "        return x + self.pe[:, :x.size(1)]\n",
    "\n",
    "class BasicTransformer(nn.Module):\n",
    "    def __init__(self, vocab_size, d_model=512, n_heads=8, n_layers=6, d_ff=2048, max_seq_length=512):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        # max_seq_length is crucial for PE and for slicing inputs during generation\n",
    "        self.max_seq_length = max_seq_length\n",
    "\n",
    "        self.embedding = nn.Embedding(vocab_size, d_model)\n",
    "        self.pos_encoding = PositionalEncoding(d_model, max_seq_length)\n",
    "\n",
    "        self.transformer_blocks = nn.ModuleList([\n",
    "            TransformerBlock(d_model, n_heads, d_ff) for _ in range(n_layers)\n",
    "        ])\n",
    "\n",
    "        self.layer_norm = nn.LayerNorm(d_model)\n",
    "        self.output_projection = nn.Linear(d_model, vocab_size)\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "\n",
    "    def create_causal_mask(self, seq_len):\n",
    "        # Creates a mask of shape (seq_len, seq_len)\n",
    "        # Lower triangle (and diagonal) is 1, upper triangle is 0.\n",
    "        mask = torch.tril(torch.ones(seq_len, seq_len))\n",
    "        # Returns shape (1, 1, seq_len, seq_len)\n",
    "        # This mask is created on CPU by default.\n",
    "        return mask.unsqueeze(0).unsqueeze(0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: (batch_size, seq_len) - input indices. Device of x depends on where model and input data are.\n",
    "        seq_len = x.size(1)\n",
    "\n",
    "        # Token embeddings and positional encoding\n",
    "        # x: (batch_size, seq_len, d_model)\n",
    "        x = self.embedding(x) * math.sqrt(self.d_model)\n",
    "        x = self.pos_encoding(x) # self.pe buffer is on the same device as model parameters\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        # Create causal mask for autoregressive generation\n",
    "        # Mask is created (likely on CPU) and then moved to x's device.\n",
    "        # x.device will be CUDA if model and inputs are on CUDA.\n",
    "        mask = self.create_causal_mask(seq_len).to(x.device)\n",
    "\n",
    "        # Pass through transformer blocks\n",
    "        for transformer_block in self.transformer_blocks:\n",
    "            x = transformer_block(x, mask) # mask is (1,1,seq_len,seq_len)\n",
    "\n",
    "        x = self.layer_norm(x)\n",
    "\n",
    "        # Project to vocabulary size\n",
    "        return self.output_projection(x)\n",
    "\n",
    "class TextProcessor:\n",
    "    def __init__(self):\n",
    "        self.word_to_idx = {}\n",
    "        self.idx_to_word = {}\n",
    "        self.vocab_size = 0\n",
    "\n",
    "    def tokenize(self, text):\n",
    "        text = text.lower()\n",
    "        tokens = re.findall(r'\\b\\w+\\b|[^\\w\\s]', text)\n",
    "        return tokens\n",
    "\n",
    "    def build_vocab(self, text, min_freq=2):\n",
    "        tokens = self.tokenize(text)\n",
    "        word_counts = Counter(tokens)\n",
    "\n",
    "        vocab = ['<pad>', '<unk>', '<start>', '<end>']\n",
    "        vocab.extend([word for word, count in word_counts.items() if count >= min_freq])\n",
    "\n",
    "        self.word_to_idx = {word: idx for idx, word in enumerate(vocab)}\n",
    "        self.idx_to_word = {idx: word for word, idx in self.word_to_idx.items()}\n",
    "        self.vocab_size = len(vocab)\n",
    "\n",
    "        print(f\"Vocabulary size: {self.vocab_size}\")\n",
    "        return vocab\n",
    "\n",
    "    def text_to_indices(self, text):\n",
    "        tokens = self.tokenize(text)\n",
    "        return [self.word_to_idx.get(token, self.word_to_idx['<unk>']) for token in tokens]\n",
    "\n",
    "    def indices_to_text(self, indices):\n",
    "        return ' '.join([self.idx_to_word.get(idx, '<unk>') for idx in indices])\n",
    "\n",
    "def create_training_data(text_indices, seq_length):\n",
    "    inputs, targets = [], []\n",
    "    for i in range(len(text_indices) - seq_length):\n",
    "        input_seq = text_indices[i:i + seq_length]\n",
    "        target_seq = text_indices[i + 1:i + seq_length + 1]\n",
    "        inputs.append(input_seq)\n",
    "        targets.append(target_seq)\n",
    "    # Tensors are created on CPU by default here\n",
    "    return torch.tensor(inputs), torch.tensor(targets)\n",
    "\n",
    "def train_model(model, train_inputs, train_targets, epochs=10, batch_size=32, lr=0.001):\n",
    "    # Uses the global `device` variable\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    criterion = nn.CrossEntropyLoss() # Can add ignore_index=processor.word_to_idx['<pad>'] if padding is used\n",
    "\n",
    "    model.train() # Set model to training mode\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        total_loss = 0\n",
    "        num_batches = 0\n",
    "\n",
    "        # Shuffle data (on CPU is fine)\n",
    "        indices = torch.randperm(len(train_inputs))\n",
    "        train_inputs_shuffled = train_inputs[indices]\n",
    "        train_targets_shuffled = train_targets[indices]\n",
    "\n",
    "        for i in range(0, len(train_inputs_shuffled), batch_size):\n",
    "            # Move batches to the target device\n",
    "            batch_inputs = train_inputs_shuffled[i:i + batch_size].to(device)\n",
    "            batch_targets = train_targets_shuffled[i:i + batch_size].to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Forward pass\n",
    "            # outputs: (batch_size, seq_len, vocab_size)\n",
    "            outputs = model(batch_inputs)\n",
    "\n",
    "            # Reshape for CrossEntropyLoss:\n",
    "            # outputs needs to be (N, C) where C = num_classes (vocab_size)\n",
    "            # targets needs to be (N)\n",
    "            loss = criterion(outputs.reshape(-1, outputs.size(-1)), batch_targets.reshape(-1))\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "            num_batches += 1\n",
    "\n",
    "        avg_loss = total_loss / num_batches\n",
    "        print(f'Epoch {epoch + 1}/{epochs}, Average Loss: {avg_loss:.4f}')\n",
    "\n",
    "def generate_text(model, processor, start_text, max_length=100, temperature=1.0):\n",
    "    # Uses the global `device` variable\n",
    "    model.eval() # Set model to evaluation mode\n",
    "\n",
    "    tokens = processor.tokenize(start_text)\n",
    "    indices = [processor.word_to_idx.get(token, processor.word_to_idx['<unk>']) for token in tokens]\n",
    "    generated_indices = indices.copy()\n",
    "\n",
    "    with torch.no_grad(): # Disable gradient calculations\n",
    "        for _ in range(max_length):\n",
    "            # Prepare input: last `model.max_seq_length` tokens\n",
    "            # model.max_seq_length is the sequence length the model was trained with\n",
    "            current_input_indices = generated_indices[-model.max_seq_length:]\n",
    "\n",
    "            input_tensor = torch.tensor([current_input_indices], dtype=torch.long).to(device)\n",
    "\n",
    "            # outputs: (1, current_seq_len, vocab_size)\n",
    "            outputs = model(input_tensor)\n",
    "\n",
    "            # Get logits for the next token prediction (after the last token in input_tensor)\n",
    "            next_token_logits = outputs[0, -1, :] / temperature\n",
    "\n",
    "            # Sample from the distribution\n",
    "            probs = F.softmax(next_token_logits, dim=-1)\n",
    "            next_token_idx = torch.multinomial(probs, 1).item()\n",
    "\n",
    "            generated_indices.append(next_token_idx)\n",
    "\n",
    "            # Stop if <end> token is generated\n",
    "            if next_token_idx == processor.word_to_idx.get('<end>', -100): # Use a dummy if <end> not in vocab\n",
    "                break\n",
    "\n",
    "    return processor.indices_to_text(generated_indices)\n",
    "\n",
    "def main():\n",
    "    # Global `device` is used by model placement, train_model, and generate_text\n",
    "    print(f\"Main function running, will use device: {device}\")\n",
    "\n",
    "    try:\n",
    "        with open('David_Copperfield.txt', 'r', encoding='utf-8') as file:\n",
    "            text = file.read()\n",
    "    except FileNotFoundError:\n",
    "        text = \"\"\"\n",
    "        David Copperfield is a novel by Charles Dickens. The story follows the life of David from childhood to maturity.\n",
    "        He faces many challenges and meets various characters along his journey. The novel explores themes of growth,\n",
    "        love, and the human condition. David learns valuable lessons about life, friendship, and perseverance.\n",
    "        Through his experiences, he develops into a mature and wise individual.\n",
    "        This is a short sample text. For better results, provide a larger corpus.\n",
    "        The model learns patterns from the data it is trained on. More data means more patterns.\n",
    "        \"\"\"\n",
    "        print(\"Using sample text. For better results, please provide 'David_Copperfield.txt' or a similar large text file.\")\n",
    "\n",
    "    processor = TextProcessor()\n",
    "    processor.build_vocab(text, min_freq=2)\n",
    "\n",
    "    text_indices = processor.text_to_indices(text)\n",
    "\n",
    "    # seq_length for training. This also becomes model.max_seq_length.\n",
    "    seq_length = 64\n",
    "    train_inputs, train_targets = create_training_data(text_indices, seq_length)\n",
    "\n",
    "    if len(train_inputs) == 0:\n",
    "        print(f\"Not enough data to create training samples. Text length (tokens): {len(text_indices)}, Sequence length: {seq_length}.\")\n",
    "        print(f\"Need at least {seq_length + 1} tokens to create one sample.\")\n",
    "        return None, None\n",
    "\n",
    "    print(f\"Training samples: {len(train_inputs)}\")\n",
    "\n",
    "    # Initialize model and move it to the determined device\n",
    "    model = BasicTransformer(\n",
    "        vocab_size=processor.vocab_size,\n",
    "        d_model=256,      # Smaller model for faster example training\n",
    "        n_heads=8,\n",
    "        n_layers=4,       # Fewer layers for faster example training\n",
    "        d_ff=1024,        # d_ff is usually 4 * d_model\n",
    "        max_seq_length=seq_length # This is important for PE and generation context\n",
    "    ).to(device)\n",
    "\n",
    "    print(f\"Model parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
    "    # Check actual device of a parameter\n",
    "    if len(list(model.parameters())) > 0:\n",
    "        print(f\"Model is on device: {next(model.parameters()).device}\")\n",
    "    else:\n",
    "        print(\"Model has no parameters.\")\n",
    "\n",
    "\n",
    "    print(\"\\nStarting training...\")\n",
    "    train_model(model, train_inputs, train_targets, epochs=1, batch_size=128, lr=0.001)\n",
    "\n",
    "    print(\"\\nGenerating text...\")\n",
    "    start_text = \"David\"\n",
    "    generated_text = generate_text(model, processor, start_text, max_length=50, temperature=0.8)\n",
    "    print(f\"Generated text: {generated_text}\")\n",
    "\n",
    "    return model, processor\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    model, processor = main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 763,
     "status": "ok",
     "timestamp": 1749204372739,
     "user": {
      "displayName": "MohammadHossein Barabadi",
      "userId": "08282318677652093529"
     },
     "user_tz": -210
    },
    "id": "Gpj_mcdJrTGt",
    "outputId": "cad09d1e-1067-44c3-c152-cc02fa376aec"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated text: my deepest sorrows are wings that she should come to see her house ; but she hurt bodily and all to me . ” “ i cannot leave her , ” said i , “ bear more than i can count , if i had had been brought up stronger than ever since . if i have been unhappy , i have loved her promise nothing , and the gentle cheerfulness way of doing anything but it ⁠ — in the <unk> ! ” “ oh , my goodness gracious sake , trotwood , ” i said , “ you ’ ll pray for me . if i couldn ’ t touch her ! ” “ i am very miserable , ” said i , “ and i never was of it\n"
     ]
    }
   ],
   "source": [
    "start_text = \"my deepest sorrows are\"\n",
    "generated_text = generate_text(model, processor, start_text, max_length=128, temperature=0.8)\n",
    "print(f\"Generated text: {generated_text}\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyPYZ0M7R4COiu5A6xQghXv6",
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
