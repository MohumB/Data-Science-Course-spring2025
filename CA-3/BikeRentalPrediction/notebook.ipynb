{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4e6e80cc",
   "metadata": {},
   "source": [
    "# Bike Rental Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7bdef576",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score, KFold\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, VotingRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score, mean_absolute_percentage_error\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "from sklearn.feature_selection import SelectKBest, f_regression, mutual_info_regression\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "093b0f7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to evaluate model performance\n",
    "def evaluate_model(y_true, y_pred, model_name):\n",
    "    mse = mean_squared_error(y_true, y_pred)\n",
    "    rmse = np.sqrt(mse)\n",
    "    mae = mean_absolute_error(y_true, y_pred)\n",
    "    r2 = r2_score(y_true, y_pred)\n",
    "    # Handle zero values in y_true to avoid division by zero in MAPE calculation\n",
    "    mask = y_true != 0\n",
    "    y_true_filtered = y_true[mask]\n",
    "    y_pred_filtered = y_pred[mask]\n",
    "    if len(y_true_filtered) > 0:\n",
    "        mape = mean_absolute_percentage_error(y_true_filtered, y_pred_filtered) * 100\n",
    "    else:\n",
    "        mape = np.nan\n",
    "    \n",
    "    print(f\"{model_name} Performance:\")\n",
    "    print(f\"MSE: {mse:.2f}\")\n",
    "    print(f\"RMSE: {rmse:.2f}\")\n",
    "    print(f\"MAE: {mae:.2f}\")\n",
    "    print(f\"R² Score: {r2:.4f}\")\n",
    "    print(f\"MAPE: {mape:.2f}%\\n\")\n",
    "    \n",
    "    return {\n",
    "        'model': model_name,\n",
    "        'mse': mse,\n",
    "        'rmse': rmse,\n",
    "        'mae': mae, \n",
    "        'r2': r2,\n",
    "        'mape': mape\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c8dea27f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the datasets\n",
    "train_data = pd.read_csv('regression-dataset-train.csv')\n",
    "test_data = pd.read_csv('regression-dataset-test-unlabeled.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "86b03519",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training dataset info:\n",
      "Shape: (510, 14)\n",
      "\n",
      "First few rows:\n",
      "    id        date  season_id  year  month  is_holiday  weekday  \\\n",
      "0  577  31-07-2019          3     1      7           0        2   \n",
      "1  427  03-03-2019          1     1      3           0        6   \n",
      "2  729  30-12-2019          1     1     12           0        0   \n",
      "3  483  28-04-2019          2     1      4           0        6   \n",
      "4  112  22-04-2018          2     0      4           0        5   \n",
      "\n",
      "   is_workingday  weather_condition  temperature  feels_like_temp  humidity  \\\n",
      "0              1                  1    29.246653          33.1448   70.4167   \n",
      "1              0                  2    16.980847          20.6746   62.1250   \n",
      "2              0                  1    10.489153          11.5850   48.3333   \n",
      "3              0                  2    15.443347          18.8752   48.9583   \n",
      "4              1                  2    13.803347          16.0977   72.9583   \n",
      "\n",
      "   wind_speed  total_users  \n",
      "0   11.083475         7216  \n",
      "1   10.792293         4066  \n",
      "2   23.500518         1796  \n",
      "3    8.708325         4220  \n",
      "4   14.707907         1683  \n",
      "\n",
      "Data types:\n",
      "id                     int64\n",
      "date                  object\n",
      "season_id              int64\n",
      "year                   int64\n",
      "month                  int64\n",
      "is_holiday             int64\n",
      "weekday                int64\n",
      "is_workingday          int64\n",
      "weather_condition      int64\n",
      "temperature          float64\n",
      "feels_like_temp      float64\n",
      "humidity             float64\n",
      "wind_speed           float64\n",
      "total_users            int64\n",
      "dtype: object\n",
      "\n",
      "Summary statistics:\n",
      "               id   season_id        year       month  is_holiday     weekday  \\\n",
      "count  510.000000  510.000000  510.000000  510.000000  510.000000  510.000000   \n",
      "mean   368.680392    2.513725    0.507843    6.541176    0.025490    2.990196   \n",
      "std    209.596164    1.110235    0.500429    3.465416    0.157763    2.017093   \n",
      "min      1.000000    1.000000    0.000000    1.000000    0.000000    0.000000   \n",
      "25%    187.250000    2.000000    0.000000    3.000000    0.000000    1.000000   \n",
      "50%    373.000000    3.000000    1.000000    7.000000    0.000000    3.000000   \n",
      "75%    552.750000    3.000000    1.000000   10.000000    0.000000    5.000000   \n",
      "max    729.000000    4.000000    1.000000   12.000000    1.000000    6.000000   \n",
      "\n",
      "       is_workingday  weather_condition  temperature  feels_like_temp  \\\n",
      "count     510.000000         510.000000   510.000000       510.000000   \n",
      "mean        0.676471           1.403922    20.108257        23.500287   \n",
      "std         0.468282           0.547888     7.431626         8.090963   \n",
      "min         0.000000           1.000000     2.424346         3.953480   \n",
      "25%         0.000000           1.000000    13.606865        16.603063   \n",
      "50%         1.000000           1.000000    20.277923        24.131150   \n",
      "75%         1.000000           2.000000    26.615847        30.177700   \n",
      "max         1.000000           3.000000    35.328347        42.044800   \n",
      "\n",
      "         humidity  wind_speed  total_users  \n",
      "count  510.000000  510.000000   510.000000  \n",
      "mean    63.123710   12.834912  4485.337255  \n",
      "std     14.153170    5.292031  1950.666077  \n",
      "min      0.000000    2.834381    22.000000  \n",
      "25%     52.270825    9.041918  3120.000000  \n",
      "50%     63.437500   12.083182  4530.000000  \n",
      "75%     73.250025   15.750879  5973.500000  \n",
      "max     97.041700   34.000021  8714.000000  \n",
      "\n",
      "Missing values:\n",
      "id                   0\n",
      "date                 0\n",
      "season_id            0\n",
      "year                 0\n",
      "month                0\n",
      "is_holiday           0\n",
      "weekday              0\n",
      "is_workingday        0\n",
      "weather_condition    0\n",
      "temperature          0\n",
      "feels_like_temp      0\n",
      "humidity             0\n",
      "wind_speed           0\n",
      "total_users          0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Display basic information about the training dataset\n",
    "print(\"Training dataset info:\")\n",
    "print(f\"Shape: {train_data.shape}\")\n",
    "print(\"\\nFirst few rows:\")\n",
    "print(train_data.head())\n",
    "print(\"\\nData types:\")\n",
    "print(train_data.dtypes)\n",
    "print(\"\\nSummary statistics:\")\n",
    "print(train_data.describe())\n",
    "print(\"\\nMissing values:\")\n",
    "print(train_data.isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "348d2258",
   "metadata": {},
   "source": [
    "## Data Exploration and Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d3c58eed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert date to datetime format\n",
    "train_data['date'] = pd.to_datetime(train_data['date'], format='%d-%m-%Y')\n",
    "test_data['date'] = pd.to_datetime(test_data['date'], format='%d-%m-%Y')\n",
    "\n",
    "# Enhanced Feature Engineering - focusing on the most important features from previous run\n",
    "for df in [train_data, test_data]:\n",
    "    # Time-based features\n",
    "    df['day'] = df['date'].dt.day\n",
    "    df['day_of_year'] = df['date'].dt.dayofyear\n",
    "    df['quarter'] = df['date'].dt.quarter\n",
    "    \n",
    "    # The previous analysis showed temperature is by far the most important feature\n",
    "    # Create better temperature-related features\n",
    "    df['temp_squared'] = df['temperature'] ** 2\n",
    "    df['temp_cubed'] = df['temperature'] ** 3\n",
    "    \n",
    "    # Weather and temperature interactions\n",
    "    df['weather_temp'] = df['weather_condition'] * df['temperature']\n",
    "    df['temp_humidity_interaction'] = df['temperature'] * df['humidity']\n",
    "    df['temp_humidity_scaled'] = df['temperature'] * (df['humidity'] / 100)\n",
    "    df['feels_temp_humidity'] = df['feels_like_temp'] * (df['humidity'] / 100)\n",
    "    \n",
    "    # Temperature difference (a significant feature from previous run)\n",
    "    df['temp_diff'] = df['temperature'] - df['feels_like_temp']\n",
    "    df['temp_diff_sq'] = df['temp_diff'] ** 2\n",
    "    \n",
    "    # Month and season interactions (important in previous run)\n",
    "    df['month_season'] = df['month'] * df['season_id']\n",
    "    df['season_temp'] = df['season_id'] * df['temperature']\n",
    "    \n",
    "    # Improved cyclic encodings for seasonal patterns\n",
    "    df['month_sin'] = np.sin(2 * np.pi * df['month']/12)\n",
    "    df['month_cos'] = np.cos(2 * np.pi * df['month']/12)\n",
    "    df['day_sin'] = np.sin(2 * np.pi * df['day']/31)\n",
    "    df['day_cos'] = np.cos(2 * np.pi * df['day']/31)\n",
    "    df['weekday_sin'] = np.sin(2 * np.pi * df['weekday']/7)\n",
    "    df['weekday_cos'] = np.cos(2 * np.pi * df['weekday']/7)\n",
    "    \n",
    "    # Day type features - Year was very important in previous run\n",
    "    df['is_weekend'] = df['weekday'].isin([5, 6]).astype(int)\n",
    "    df['peak_season'] = ((df['month'] >= 6) & (df['month'] <= 9)).astype(int)\n",
    "    df['season_weekend'] = df['is_weekend'] * df['season_id']\n",
    "    df['year_season'] = df['year'] * df['season_id']\n",
    "    \n",
    "    # High wind indicator - create thresholds based on data\n",
    "    df['high_wind'] = (df['wind_speed'] > 15).astype(int)\n",
    "    df['extreme_weather'] = (df['weather_condition'] >= 2).astype(int)\n",
    "    \n",
    "    # Log transform for potential non-linear relationships\n",
    "    df['log_temp'] = np.log1p(df['temperature'] - df['temperature'].min() + 1)\n",
    "\n",
    "# Feature selection - Focus on temperature, year, season as they were most important\n",
    "# Extract key features for analysis\n",
    "key_features = ['temperature', 'year', 'season_id', 'weather_temp', 'feels_like_temp', \n",
    "                'humidity', 'day_of_year', 'month_season', 'wind_speed', \n",
    "                'temp_humidity_interaction', 'temp_diff', 'month_sin', 'month_cos']\n",
    "\n",
    "# Visualize correlation of key features with target\n",
    "plt.figure(figsize=(14, 10))\n",
    "key_corr = pd.concat([train_data[key_features], train_data['total_users']], axis=1).corr()['total_users'].sort_values(ascending=False)\n",
    "sns.barplot(x=key_corr.values[1:], y=key_corr.index[1:])\n",
    "plt.title('Correlation with Total Users (Key Features)')\n",
    "plt.tight_layout()\n",
    "plt.savefig('key_features_correlation.png')\n",
    "plt.close()\n",
    "\n",
    "# Visualize temperature vs total users with improved trend\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.scatterplot(x='temperature', y='total_users', hue='year', data=train_data, alpha=0.7)\n",
    "sns.regplot(x='temperature', y='total_users', data=train_data, scatter=False, color='red')\n",
    "plt.title('Temperature vs Total Users with Year Grouping')\n",
    "plt.savefig('temp_users_by_year.png')\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35ab8080",
   "metadata": {},
   "source": [
    "## Model Training and Evaluation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7146cdb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating Gradient Boosting model with cross-validation:\n",
      "Performing 5-fold cross-validation...\n",
      "Cross-validation results:\n",
      "Mean MSE: 455688.48 (±97663.00)\n",
      "Mean RMSE: 671.05 (±73.37)\n",
      "Mean MAE: 470.46 (±48.30)\n",
      "Mean R²: 0.8758 (±0.0301)\n",
      "Mean MAPE: 43.41% (±56.73%)\n",
      "\n",
      "Evaluating Ensemble model with cross-validation:\n",
      "Performing 5-fold cross-validation...\n",
      "Cross-validation results:\n",
      "Mean MSE: 440839.49 (±107001.09)\n",
      "Mean RMSE: 658.82 (±82.46)\n",
      "Mean MAE: 457.53 (±56.53)\n",
      "Mean R²: 0.8801 (±0.0307)\n",
      "Mean MAPE: 42.63% (±55.74%)\n",
      "\n",
      "Training and evaluating on validation set...\n",
      "Gradient Boosting Performance:\n",
      "MSE: 458707.56\n",
      "RMSE: 677.28\n",
      "MAE: 431.19\n",
      "R² Score: 0.8513\n",
      "MAPE: 14.43%\n",
      "\n",
      "Ensemble Performance:\n",
      "MSE: 448829.35\n",
      "RMSE: 669.95\n",
      "MAE: 421.20\n",
      "R² Score: 0.8545\n",
      "MAPE: 14.32%\n",
      "\n",
      "\n",
      "Selected model: Ensemble\n"
     ]
    }
   ],
   "source": [
    "# Split the data\n",
    "X = train_data.drop(['id', 'date', 'total_users'], axis=1)\n",
    "y = train_data['total_users']\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Select categorical and numerical columns\n",
    "categorical_cols = X.select_dtypes(include=['object', 'category']).columns.tolist()\n",
    "numerical_cols = X.select_dtypes(include=[np.number]).columns.tolist()\n",
    "\n",
    "# Create preprocessor\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', Pipeline([\n",
    "            ('imputer', SimpleImputer(strategy='median')),\n",
    "            ('scaler', StandardScaler())\n",
    "        ]), numerical_cols),\n",
    "        ('cat', Pipeline([\n",
    "            ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "            ('encoder', OneHotEncoder(handle_unknown='ignore'))\n",
    "        ]), categorical_cols)\n",
    "    ])\n",
    "\n",
    "# Define models for ensemble\n",
    "gb_model = GradientBoostingRegressor(\n",
    "    n_estimators=200,\n",
    "    learning_rate=0.1,\n",
    "    max_depth=3,\n",
    "    min_samples_split=5,\n",
    "    min_samples_leaf=2,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "xgb_model = xgb.XGBRegressor(\n",
    "    n_estimators=200,\n",
    "    learning_rate=0.1,\n",
    "    max_depth=4,\n",
    "    subsample=0.8,\n",
    "    colsample_bytree=0.9,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "lgb_model = lgb.LGBMRegressor(\n",
    "    n_estimators=200,\n",
    "    learning_rate=0.1,\n",
    "    max_depth=3,\n",
    "    num_leaves=31,\n",
    "    random_state=42,\n",
    "    verbose=-1  # Suppress verbose output\n",
    ")\n",
    "\n",
    "# Create ensemble model\n",
    "ensemble = VotingRegressor(\n",
    "    estimators=[\n",
    "        ('gb', gb_model),\n",
    "        ('xgb', xgb_model),\n",
    "        ('lgb', lgb_model)\n",
    "    ],\n",
    "    weights=[0.4, 0.35, 0.25]  # Weight based on previous performance, with GB having highest weight\n",
    ")\n",
    "\n",
    "# Create pipeline with ensemble\n",
    "ensemble_pipeline = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('model', ensemble)\n",
    "])\n",
    "\n",
    "# Create pipeline with gradient boosting (our previously best model)\n",
    "gb_pipeline = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('model', gb_model)\n",
    "])\n",
    "\n",
    "# Function to perform cross-validation with multiple metrics\n",
    "def cross_validate_with_metrics(pipeline, X, y, cv=5):\n",
    "    print(f\"Performing {cv}-fold cross-validation...\")\n",
    "    kf = KFold(n_splits=cv, shuffle=True, random_state=42)\n",
    "    \n",
    "    mse_scores = []\n",
    "    rmse_scores = []\n",
    "    mae_scores = []\n",
    "    r2_scores = []\n",
    "    mape_scores = []\n",
    "    \n",
    "    for train_idx, val_idx in kf.split(X):\n",
    "        X_train_cv, X_val_cv = X.iloc[train_idx], X.iloc[val_idx]\n",
    "        y_train_cv, y_val_cv = y.iloc[train_idx], y.iloc[val_idx]\n",
    "        \n",
    "        pipeline.fit(X_train_cv, y_train_cv)\n",
    "        y_pred_cv = pipeline.predict(X_val_cv)\n",
    "        \n",
    "        mse_scores.append(mean_squared_error(y_val_cv, y_pred_cv))\n",
    "        rmse_scores.append(np.sqrt(mean_squared_error(y_val_cv, y_pred_cv)))\n",
    "        mae_scores.append(mean_absolute_error(y_val_cv, y_pred_cv))\n",
    "        r2_scores.append(r2_score(y_val_cv, y_pred_cv))\n",
    "        \n",
    "        # Handle zero values in y_true to avoid division by zero in MAPE calculation\n",
    "        mask = y_val_cv != 0\n",
    "        y_val_cv_filtered = y_val_cv[mask]\n",
    "        y_pred_cv_filtered = y_pred_cv[mask]\n",
    "        if len(y_val_cv_filtered) > 0:\n",
    "            mape_scores.append(mean_absolute_percentage_error(y_val_cv_filtered, y_pred_cv_filtered) * 100)\n",
    "        else:\n",
    "            mape_scores.append(np.nan)\n",
    "    \n",
    "    print(f\"Cross-validation results:\")\n",
    "    print(f\"Mean MSE: {np.mean(mse_scores):.2f} (±{np.std(mse_scores):.2f})\")\n",
    "    print(f\"Mean RMSE: {np.mean(rmse_scores):.2f} (±{np.std(rmse_scores):.2f})\")\n",
    "    print(f\"Mean MAE: {np.mean(mae_scores):.2f} (±{np.std(mae_scores):.2f})\")\n",
    "    print(f\"Mean R²: {np.mean(r2_scores):.4f} (±{np.std(r2_scores):.4f})\")\n",
    "    print(f\"Mean MAPE: {np.mean(mape_scores):.2f}% (±{np.std(mape_scores):.2f}%)\")\n",
    "    print()\n",
    "    \n",
    "    return {\n",
    "        'mse': np.mean(mse_scores),\n",
    "        'rmse': np.mean(rmse_scores),\n",
    "        'mae': np.mean(mae_scores),\n",
    "        'r2': np.mean(r2_scores),\n",
    "        'mape': np.mean(mape_scores)\n",
    "    }\n",
    "\n",
    "# Evaluate both models with cross-validation\n",
    "print(\"Evaluating Gradient Boosting model with cross-validation:\")\n",
    "gb_cv_results = cross_validate_with_metrics(gb_pipeline, X, y, cv=5)\n",
    "\n",
    "print(\"Evaluating Ensemble model with cross-validation:\")\n",
    "ensemble_cv_results = cross_validate_with_metrics(ensemble_pipeline, X, y, cv=5)\n",
    "\n",
    "# Train and evaluate on validation set\n",
    "print(\"Training and evaluating on validation set...\")\n",
    "gb_pipeline.fit(X_train, y_train)\n",
    "y_pred_gb = gb_pipeline.predict(X_val)\n",
    "gb_results = evaluate_model(y_val, y_pred_gb, \"Gradient Boosting\")\n",
    "\n",
    "ensemble_pipeline.fit(X_train, y_train)\n",
    "y_pred_ensemble = ensemble_pipeline.predict(X_val)\n",
    "ensemble_results = evaluate_model(y_val, y_pred_ensemble, \"Ensemble\")\n",
    "\n",
    "# Choose the best model\n",
    "best_model_pipeline = gb_pipeline if gb_results['mse'] < ensemble_results['mse'] else ensemble_pipeline\n",
    "best_model_name = \"Gradient Boosting\" if gb_results['mse'] < ensemble_results['mse'] else \"Ensemble\"\n",
    "print(f\"\\nSelected model: {best_model_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "738c42b0",
   "metadata": {},
   "source": [
    "## Hyper Parameter Fine-Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7d425747",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fine-tuning Ensemble model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/codespace/.local/lib/python3.12/site-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "/home/codespace/.local/lib/python3.12/site-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "/home/codespace/.local/lib/python3.12/site-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "/home/codespace/.local/lib/python3.12/site-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "/home/codespace/.local/lib/python3.12/site-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "/home/codespace/.local/lib/python3.12/site-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "/home/codespace/.local/lib/python3.12/site-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "/home/codespace/.local/lib/python3.12/site-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "/home/codespace/.local/lib/python3.12/site-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters: {'model__weights': [0.35, 0.4, 0.25]}\n",
      "Best CV score: 457334.90 (MSE)\n"
     ]
    }
   ],
   "source": [
    "if best_model_name == \"Gradient Boosting\":\n",
    "    # Define more refined parameter grid for GB\n",
    "    param_grid = {\n",
    "        'model__n_estimators': [150, 200, 250],\n",
    "        'model__learning_rate': [0.05, 0.1, 0.15],\n",
    "        'model__max_depth': [3, 4],\n",
    "        'model__min_samples_split': [2, 5],\n",
    "        'model__min_samples_leaf': [1, 2],\n",
    "        'model__subsample': [0.8, 0.9, 1.0]\n",
    "    }\n",
    "    \n",
    "    # Create pipeline for tuning\n",
    "    tuning_pipeline = Pipeline([\n",
    "        ('preprocessor', preprocessor),\n",
    "        ('model', GradientBoostingRegressor(random_state=42))\n",
    "    ])\n",
    "else:\n",
    "    # If ensemble is best, fine-tune the weights\n",
    "    param_grid = {\n",
    "        'model__weights': [[0.4, 0.35, 0.25], [0.45, 0.3, 0.25], [0.35, 0.4, 0.25]]\n",
    "    }\n",
    "    \n",
    "    # Create tuning pipeline for ensemble\n",
    "    tuning_pipeline = Pipeline([\n",
    "        ('preprocessor', preprocessor),\n",
    "        ('model', VotingRegressor(\n",
    "            estimators=[\n",
    "                ('gb', gb_model),\n",
    "                ('xgb', xgb_model),\n",
    "                ('lgb', lgb_model)\n",
    "            ]\n",
    "        ))\n",
    "    ])\n",
    "\n",
    "print(f\"Fine-tuning {best_model_name} model...\")\n",
    "grid_search = GridSearchCV(\n",
    "    tuning_pipeline,\n",
    "    param_grid=param_grid,\n",
    "    cv=3,  # Reduced CV to speed up tuning\n",
    "    scoring='neg_mean_squared_error',\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "grid_search.fit(X, y)  # Use full training data\n",
    "\n",
    "print(f\"Best parameters: {grid_search.best_params_}\")\n",
    "print(f\"Best CV score: {-grid_search.best_score_:.2f} (MSE)\")\n",
    "\n",
    "# Final model with best parameters\n",
    "final_pipeline = grid_search.best_estimator_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6865922",
   "metadata": {},
   "source": [
    "## Training Final Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e160bcb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Feature importances from Gradient Boosting (part of ensemble):\n",
      "Top 15 features by importance:\n",
      "                Feature  Importance\n",
      "33          year_season    0.472356\n",
      "14         temp_squared    0.064798\n",
      "36             log_temp    0.054112\n",
      "7           temperature    0.050813\n",
      "16         weather_temp    0.043942\n",
      "8       feels_like_temp    0.040307\n",
      "15           temp_cubed    0.039163\n",
      "1                  year    0.035747\n",
      "9              humidity    0.035517\n",
      "23          season_temp    0.032538\n",
      "10           wind_speed    0.025755\n",
      "21         temp_diff_sq    0.015578\n",
      "6     weather_condition    0.014072\n",
      "12          day_of_year    0.011839\n",
      "19  feels_temp_humidity    0.010171\n"
     ]
    }
   ],
   "source": [
    "final_pipeline.fit(X, y)\n",
    "\n",
    "# Feature importance analysis for GradientBoosting (if applicable)\n",
    "if hasattr(final_pipeline.named_steps['model'], 'feature_importances_'):\n",
    "    print(\"\\nFeature importances:\")\n",
    "    importances = final_pipeline.named_steps['model'].feature_importances_\n",
    "    feature_names = numerical_cols + categorical_cols\n",
    "    \n",
    "    feature_importance = pd.DataFrame({\n",
    "        'Feature': feature_names[:len(importances)] if len(feature_names) >= len(importances) else feature_names + ['Unknown'] * (len(importances) - len(feature_names)),\n",
    "        'Importance': importances\n",
    "    }).sort_values('Importance', ascending=False)\n",
    "    \n",
    "    print(\"Top 15 features by importance:\")\n",
    "    print(feature_importance.head(15))\n",
    "    \n",
    "    # Plot feature importance\n",
    "    plt.figure(figsize=(12, 10))\n",
    "    sns.barplot(x='Importance', y='Feature', data=feature_importance.head(15))\n",
    "    plt.title(f'Feature Importance ({best_model_name})')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('final_feature_importance.png')\n",
    "    plt.close()\n",
    "elif isinstance(final_pipeline.named_steps['model'], VotingRegressor):\n",
    "    # For ensemble, get feature importance from the first estimator (GB)\n",
    "    if hasattr(final_pipeline.named_steps['model'].estimators_[0], 'feature_importances_'):\n",
    "        print(\"\\nFeature importances from Gradient Boosting (part of ensemble):\")\n",
    "        importances = final_pipeline.named_steps['model'].estimators_[0].feature_importances_\n",
    "        feature_names = numerical_cols + categorical_cols\n",
    "        \n",
    "        feature_importance = pd.DataFrame({\n",
    "            'Feature': feature_names[:len(importances)] if len(feature_names) >= len(importances) else feature_names + ['Unknown'] * (len(importances) - len(feature_names)),\n",
    "            'Importance': importances\n",
    "        }).sort_values('Importance', ascending=False)\n",
    "        \n",
    "        print(\"Top 15 features by importance:\")\n",
    "        print(feature_importance.head(15))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8970b4d",
   "metadata": {},
   "source": [
    "## Generating Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fbc65110",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions saved to 'regression_predictions.csv'\n",
      "\n",
      "Submission summary statistics:\n",
      "count     220.000000\n",
      "mean     4441.380711\n",
      "std      1729.363251\n",
      "min       771.264054\n",
      "25%      3321.255112\n",
      "50%      4508.747255\n",
      "75%      5797.807723\n",
      "max      8096.700788\n",
      "Name: label, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "test_predictions = final_pipeline.predict(test_data.drop(['id', 'date'], axis=1))\n",
    "\n",
    "# Create submission file\n",
    "submission = pd.DataFrame({\n",
    "    'id': test_data['id'],\n",
    "    'label': test_predictions\n",
    "})\n",
    "\n",
    "# Ensure predictions are non-negative\n",
    "submission['label'] = submission['label'].clip(0)\n",
    "\n",
    "# Save to CSV\n",
    "submission.to_csv('regression_predictions.csv', index=False)\n",
    "print(f\"Predictions saved to 'regression_predictions.csv'\")\n",
    "\n",
    "# Print submission summary statistics\n",
    "print(\"\\nSubmission summary statistics:\")\n",
    "print(submission['label'].describe())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
