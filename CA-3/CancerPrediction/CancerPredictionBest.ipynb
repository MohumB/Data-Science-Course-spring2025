# %% [markdown]
# <h1>Cancer Prediction Task</h1>

# %%
import pandas as pd

# %%
TRAIN_DATA_PATH = r"D:\Universite\Term8\Data Science\Data-Science-Course-spring2025\CA-3\CancerPrediction\train_data.csv"
TEST_DATA_PATH = r"D:\Universite\Term8\Data Science\Data-Science-Course-spring2025\CA-3\CancerPrediction\test_data.csv"
df = pd.read_csv(TRAIN_DATA_PATH)
test_df = pd.read_csv(TEST_DATA_PATH)
df.info()
df.head()

# %% [markdown]
# Some Preparations Required, like some Type Conversions

# %%
def preprocess_data(df):
    df['Height'] = df['Height'].str.extract('(\d+\.?\d*)')[0].astype('float64')
    df['Birth_Date'] = pd.to_datetime(df['Birth_Date'])
    df['Diagnosis_Date'] = pd.to_datetime(df['Diagnosis_Date'])
    df['Surgery_Date'] = pd.to_datetime(df['Surgery_Date'])

    df['Age_at_Diagnosis'] = ((df['Diagnosis_Date'] - df['Birth_Date']).dt.days / 365.25).round()

    df['Days_to_Surgery'] = (df['Surgery_Date'] - df['Diagnosis_Date']).dt.days
    binary_cols = ['Recurrence_Status', 'Targeted_Therapy', 'Immunotherapy', 'Family_History']
    
    for col in binary_cols:
        df[col] = df[col].map({'Yes': 1, 'No': 0, 'NO':0})
        
    stage_mapping = {"I": 1, "II": 2, "III": 3, "IV": 4}
    df['Stage_Encoded'] = df['Stage_at_Diagnosis'].map(stage_mapping)
    
    smoking_mapping = {"Never": 0, "Former": 1, "Current": 2}
    df['Smoking_Encoded'] = df['Smoking_History'].map(smoking_mapping)
    
    alcohol_mapping = {"Never" : 0, "Occasional" : 1, "Regular" : 2}
    df['Alcohol_Use_Encoded'] = df['Alcohol_Use'].map(alcohol_mapping)
    df['Symptoms'].fillna('Unknown', inplace = True)
    df['Chemotherapy_Drugs'].fillna('None', inplace = True)
    median = df['Days_to_Surgery'].median()
    df['Days_to_Surgery'].fillna(value = median, inplace = True)
    df['Chemo_Radiation'] = ((df['Chemotherapy_Drugs'] != 'None') & (df['Radiation_Sessions'] 
                 > 0)).astype(int)

    df['Symptom_Count'] = df['Symptoms'].str.split(',').apply(lambda x: len(x) if 
                    isinstance(x, list) else 0)
    df['BMI'] = (df['Weight'] / ((df['Height'] / 100) ** 2)).round()
    import numpy as np
    df['Surgery_Delay_Category'] = pd.cut(df['Days_to_Surgery'], 
                                        bins = [0, 30, 90, 365, np.inf],
                                    labels = ['Immidiate', 'Short', 'Medium', 'Long'])
    df = df.drop(['Weight', 'Height', 'Days_to_Surgery'], axis=1)
    df = df.drop(['Birth_Date', 'Diagnosis_Date', 'Surgery_Date', 'Stage_at_Diagnosis',
             'Smoking_History', 'Alcohol_Use'], axis = 1)

    return df


# %%
train_df = preprocess_data(df)
test_df = preprocess_data(test_df)

# %% [markdown]
# Convertion of Yes/ No to binary values with yes corresponding to 1, and no corresponding to 0

# %% [markdown]
# Encode Some Columns (one-hot-encoding)

# %%
train_df.head()

# %%


# %%
train_df.isnull().any()


# %%
train_df.head()

# %%
train_df.info()

# %%


# %%
train_df.head()

# %% [markdown]
# <h2>1. Data Exploration and Understanding</h2>

# %% [markdown]
# **1.1. Cross Tabulation for Some Features, both row_wise and col_wise**

# %%
def cross_tab(df, col1, col2, normalize_option):
    ct = pd.crosstab(df[col1], df[col2])
    ct_pct = pd.crosstab(df[col1], df[col2], normalize = normalize_option) # column-wise
    return ct_pct

features = ['Urban_Rural', 'Family_History', 'Stage_Encoded', 'Immunotherapy',
            'Targeted_Therapy', 'Recurrence_Status', 'Smoking_Encoded', 'Alcohol_Use_Encoded']

for feature in features:
    ct = cross_tab(train_df, feature, 'label', 'index') # row-wise
    print(ct)
    print("\n")

# %%
for feature in features:
    ct = cross_tab(train_df, feature, 'label', 'columns') # col-wise
    print(ct)
    print("\n")

# %% [markdown]
# **1.2. Numerical Features Analysis**

# %%
import matplotlib.pyplot as plt
from scipy import stats
import seaborn as sns
def numerical_feature_analysis(df, feature):
    print(f"Feature: {feature}")
    print(f"Mean: {df[feature].mean()}")
    print(f"Median: {df[feature].median()}")
    print(f"Standard Deviation: {df[feature].std()}")
    print(f"Variance: {df[feature].var()}")
    print(f"Minimum: {df[feature].min()}")
    print(f"Maximum: {df[feature].max()}")
    print("\n")
    
    sns.countplot(x = feature, data = df, hue = 'label')
    plt.title(f"Countplot of {feature} by label")
    plt.show()
    
    alive = df[df['label'] == 1][feature]
    deceased = df[df['label'] == 0][feature]
    t_stat, p_val = stats.ttest_ind(alive, deceased, nan_policy='omit')
    print(f"{feature}: t-stat={t_stat:.2f}, p-value={p_val:.10f}")

# %%
numerical_features = ['Radiation_Sessions', 'Tumor_Size']
for col in numerical_features:
    numerical_feature_analysis(train_df, col) 
    print("\n")

# %% [markdown]
# Features that are highly correlated: Tumor Size and Radiation_Sessions

# %% [markdown]
# **Further Feature Engineering**

# %% [markdown]
# Because Weight and Height are highly correlated, I will make a BMI metric out of it

# %%
corr_matrix = train_df.select_dtypes(include=['number']).corr()
threshhold = 0.05
  
sns.heatmap(corr_matrix, annot=True, cmap='coolwarm')

label_corr = corr_matrix['label'].abs()  # Absolute correlation values
low_corr_features = label_corr[label_corr < threshhold].index.tolist()

# Drop low-correlation features from DataFrame
train_df = train_df.drop(columns=low_corr_features)
low_corr_features = low_corr_features[1:]
test_df = test_df.drop(columns=low_corr_features) 

# %%
low_corr_features

# %% [markdown]
# <h2>Different Models</h2>

# %% [markdown]
# <h3>SVM</h3>

# %%
from sklearn.svm import SVC
from sklearn.metrics import classification_report, accuracy_score, f1_score, precision_score, recall_score
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler, OneHotEncoder
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.impute import SimpleImputer

# %%
X_train = train_df.drop('label', axis = 1)
y_train = train_df['label']

X_test = test_df
# y_test = test_df['label']

# %%
train_df.info()

# %%
numeric_features = ['Tumor_Size', 'Radiation_Sessions',
        'Age_at_Diagnosis', 'BMI']
for col in numeric_features:
    df[col] = pd.to_numeric(df[col], errors='coerce')
    
categorical_features = ['Urban_Rural', 'Occupation', 'Insurance_Type',
           'Symptoms', 'Chemotherapy_Drugs', 'Cancer_Type', 'Surgery_Delay_Category'             
]
encoded_features = ['Stage_Encoded',
                    'Smoking_Encoded',
                    'Alcohol_Use_Encoded']

# %%
# numeric_features = [
#         'Age_at_Diagnosis', 'BMI']
# for col in numeric_features:
#     df[col] = pd.to_numeric(df[col], errors='coerce')
    
# categorical_features = ['Urban_Rural', 'Occupation', 'Insurance_Type',
#            'Symptoms', 'Chemotherapy_Drugs', 'Cancer_Type', 'Surgery_Delay_Category'             
# ]
# encoded_features = ['Stage_Encoded']

# %%
numeric_transformer = Pipeline(steps = [
    ('imputer', SimpleImputer(strategy='median')),
    ('scaler', StandardScaler())
])

categorical_transformer = Pipeline(steps=[
     ('imputer', SimpleImputer(strategy='constant', fill_value='Unknown')),
    ('onehot', OneHotEncoder(handle_unknown = 'ignore'))
])

preprocessor = ColumnTransformer(
    transformers = [
        ('num', numeric_transformer, numeric_features),
        ('cat', categorical_transformer, categorical_features)
    ],
    remainder = 'passthrough'
)


# %% [markdown]
# <h3>Experiment with Best Model Prediction</h3>

# %%
from lightgbm import LGBMClassifier
from catboost import CatBoostClassifier
from xgboost import XGBClassifier
from sklearn.model_selection import GridSearchCV
from sklearn.metrics import accuracy_score
import joblib

# 1. Define all model pipelines
models = {
    'XGBoost': Pipeline([
        ('preprocessor', preprocessor),
        ('classifier', XGBClassifier(random_state=42))
    ]),
    'LightGBM': Pipeline([
        ('preprocessor', preprocessor),
        ('classifier', LGBMClassifier(random_state=42))
    ]),
    'CatBoost': Pipeline([
        ('preprocessor', preprocessor),
        ('classifier', CatBoostClassifier(random_state=42, verbose=0))
    ])
}

# 2. Define parameter grids for each model
param_grids = {
    'XGBoost': {
        'classifier__n_estimators': [100, 200],
        'classifier__max_depth': [3, 5, 7],
        'classifier__learning_rate': [0.01, 0.1],
        'classifier__subsample': [0.6, 0.8],
        'classifier__colsample_bytree': [0.6, 0.8]
    },
    'LightGBM': {
        'classifier__n_estimators': [100, 200],
        'classifier__max_depth': [3, 5, 7],
        'classifier__learning_rate': [0.01, 0.1],
        'classifier__subsample': [0.6, 0.8],
        'classifier__colsample_bytree': [0.6, 0.8]
    },
    'CatBoost': {
        'classifier__iterations': [100, 200],
        'classifier__depth': [3, 5, 7],
        'classifier__learning_rate': [0.01, 0.1],
        'classifier__l2_leaf_reg': [1, 3]
    }
}

# 3. Create validation set from training data
from sklearn.model_selection import train_test_split
X_train, X_val, y_train, y_val = train_test_split(
    X_train, y_train, test_size=0.2, random_state=42
)

# 4. Train and evaluate all models
best_models = {}
best_scores = {}

for name in models.keys():
    print(f"\nTraining {name}...")
    grid_search = GridSearchCV(
        models[name],
        param_grids[name],
        cv=3,
        scoring='accuracy',
        n_jobs=-1,
        verbose=1
    )
    grid_search.fit(X_train, y_train)
    
    # Get best model and score
    best_models[name] = grid_search.best_estimator_
    val_score = accuracy_score(y_val, best_models[name].predict(X_val))
    best_scores[name] = val_score
    print(f"{name} best validation accuracy: {val_score:.4f}")
    
    # Save model
    joblib.dump(best_models[name], f'{name}_best_model.pkl')

# 5. Select best model
best_model_name = max(best_scores, key=best_scores.get)
best_model = best_models[best_model_name]
print(f"\nBest model: {best_model_name} with accuracy {best_scores[best_model_name]:.4f}")

# 6. Retrain best model on full training data
best_model.fit(pd.concat([X_train, X_val]), pd.concat([y_train, y_val]))

# 7. Make predictions and save
y_pred = best_model.predict(X_test)
result = pd.DataFrame({'id': test_df['id'], 'label': y_pred})
RESULT_PATH = r"D:\Universite\Term8\Data Science\Data-Science-Course-spring2025\CA-3\CancerPrediction\result.csv"

result.to_csv(RESULT_PATH, index=False)
print(f"Predictions saved to {RESULT_PATH} using {best_model_name}")

